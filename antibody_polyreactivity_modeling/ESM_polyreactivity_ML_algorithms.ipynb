{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score, precision_recall_fscore_support, cohen_kappa_score, make_scorer, roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "import random\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "ESM2_MODEL_NAMES = [\n",
    "    \"facebook/esm2_t6_8M_UR50D\",\n",
    "    \"facebook/esm2_t12_35M_UR50D\",\n",
    "    \"facebook/esm2_t30_150M_UR50D\",\n",
    "    \"facebook/esm2_t33_650M_UR50D\",\n",
    "]\n",
    "MAX_LENGTH = 256\n",
    "EMBEDDING_BATCH_SIZE = 16\n",
    "NUM_CLASSES = 3\n",
    "\n",
    "SEED = 42\n",
    "N_SPLITS_ML = 5\n",
    "N_ITER_RANDOM_SEARCH = 30\n",
    "CV_RANDOM_SEARCH = 3\n",
    "\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "elif torch.backends.mps.is_available():\n",
    "    pass\n",
    "torch.manual_seed(SEED)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS (Apple Silicon GPU) for embedding generation.\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA for embedding generation.\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU for embedding generation.\")\n",
    "\n",
    "def load_and_preprocess_data_for_embeddings(csv_path='data.csv'):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file {csv_path} was not found.\")\n",
    "        print(\"Creating a dummy DataFrame for demonstration purposes.\")\n",
    "        data = {\n",
    "            'VH': [\"EVQLVESGGGLVQPGGSLRLSCAASGFTFSSYAMSWVRQAPGKGLEWVSAISGSGGSTYYADSVKGRFTISRDNSKNTLYLQMNSLRAEDTAVYYCAKDRLGRYFDYWGQGTLVTVSS\",\n",
    "                   \"QVQLQESGPGLVKPSQTLSLTCTVSGGSISSYYWSWIRQPPGKGLEWIGYIYYSGSTYYNPSLKSRVTISVDTSKNQFSLKLSSVTAADTAVYYCARWDYLRDYWGQGTLVTVSS\"] * 5,\n",
    "            'VL': [\"DIQMTQSPSSLSASVGDRVTITCRASQGISSALAWYQQKPGKAPKLLIYDASSLESGVPSRFSGSGSGTDFTLTISSLQPEDFATYYCQQFNSYPLTFGGGTKVEIK\",\n",
    "                   \"EIVLTQSPGTLSLSPGERATLSCRASQSVSSSYLAWYQQKPGQAPRLLIYGASSRATGIPDRFSGSGSGTDFTLTISRLEPEDFAVYYCQQYGSSPTFGGGTKVEIK\"] * 5,\n",
    "            'psr': [0.05, 0.25, 0.55, 0.08, 0.15, 0.02, 0.30, 0.60, 0.09, 0.20] \n",
    "        }\n",
    "        df = pd.DataFrame(data)\n",
    "        chain_max_len = MAX_LENGTH // 2 - 2 \n",
    "        df['VH'] = df['VH'].apply(lambda x: x[:chain_max_len]) \n",
    "        df['VL'] = df['VL'].apply(lambda x: x[:chain_max_len])\n",
    "        df = pd.concat([df]*2, ignore_index=True)\n",
    "\n",
    "    if not all(col in df.columns for col in ['VH', 'VL', 'psr']):\n",
    "        raise ValueError(\"DataFrame must contain 'VH', 'VL', and 'psr' columns.\")\n",
    "\n",
    "    \n",
    "    df['combined_sequence'] = df['VH'] + 'X' + df['VL']\n",
    "    \n",
    "    \n",
    "    print(f\"Total sequences: {len(df)}\")\n",
    "    print(f\"Label distribution:\\n{df['label'].value_counts().sort_index()}\")\n",
    "    \n",
    "    return df['combined_sequence'].tolist(), df['label'].to_numpy()\n",
    "\n",
    "def generate_embeddings_batched(sequences, esm_model_name, tokenizer_name, device, max_len, batch_size_embed):\n",
    "    print(f\"\\nLoading ESM model: {esm_model_name} for embedding generation...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "    model = AutoModel.from_pretrained(esm_model_name)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    all_pooled_embeddings = []\n",
    "    num_sequences = len(sequences)\n",
    "    \n",
    "    print(f\"Generating Attention Pooled embeddings for {num_sequences} sequences with batch size {batch_size_embed}...\")\n",
    "    for i in range(0, num_sequences, batch_size_embed):\n",
    "        batch_sequences = sequences[i:i+batch_size_embed]\n",
    "        \n",
    "        inputs = tokenizer.batch_encode_plus(\n",
    "            batch_sequences, add_special_tokens=True, max_length=max_len,\n",
    "            padding='max_length', truncation=True, return_tensors='pt',\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        \n",
    "        input_ids = inputs['input_ids'].to(device)\n",
    "        attention_mask = inputs['attention_mask'].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "            token_embeddings = outputs.last_hidden_state\n",
    "            \n",
    "            input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "            sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "            sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9) \n",
    "            pooled_embeddings_batch = (sum_embeddings / sum_mask).cpu().numpy()\n",
    "        \n",
    "        all_pooled_embeddings.append(pooled_embeddings_batch)\n",
    "        \n",
    "        if (i // batch_size_embed) % 5 == 0 or i + batch_size_embed >= num_sequences : \n",
    "            print(f\"  Processed batch {i // batch_size_embed + 1}/{(num_sequences + batch_size_embed -1) // batch_size_embed}\")\n",
    "\n",
    "        if device.type == 'mps': torch.mps.empty_cache()\n",
    "        del input_ids, attention_mask, outputs, token_embeddings, input_mask_expanded, sum_embeddings, sum_mask, pooled_embeddings_batch\n",
    "        gc.collect()\n",
    "        \n",
    "    print(\"Embedding generation complete.\")\n",
    "    del model, tokenizer \n",
    "    if device.type == 'cuda': torch.cuda.empty_cache()\n",
    "    if device.type == 'mps': torch.mps.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return np.concatenate(all_pooled_embeddings, axis=0)\n",
    "\n",
    "def make_pipe(preprocessor, clf):\n",
    "    return Pipeline([(\"prep\", preprocessor), (\"clf\", clf)])\n",
    "\n",
    "all_sequences, y_labels = load_and_preprocess_data_for_embeddings(csv_path='data.csv') \n",
    "overall_esm_results = []\n",
    "\n",
    "for esm_name in ESM2_MODEL_NAMES:\n",
    "    print(f\"\\n\\n{'='*20} Processing ESM Model: {esm_name} {'='*20}\")\n",
    "    \n",
    "    tokenizer_to_use = esm_name \n",
    "    X_embeddings = generate_embeddings_batched(all_sequences, esm_name, tokenizer_to_use, device, MAX_LENGTH, EMBEDDING_BATCH_SIZE)\n",
    "    \n",
    "    print(f\"Generated embeddings shape for {esm_name}: {X_embeddings.shape}\")\n",
    "    print(f\"Labels shape: {y_labels.shape}\")\n",
    "\n",
    "    current_embedding_dim = X_embeddings.shape[1]\n",
    "    n_svd_components = min(150, current_embedding_dim - 1 if current_embedding_dim > 1 else 1)\n",
    "    if current_embedding_dim <=1: \n",
    "        print(f\"Warning: Embedding dimension {current_embedding_dim} is too low for SVD. Skipping SVD.\")\n",
    "        preproc_pipeline = Pipeline([(\"scaler\", StandardScaler())])\n",
    "    else:\n",
    "        print(f\"Using TruncatedSVD with n_components={n_svd_components} for {esm_name}\")\n",
    "        preproc_pipeline = Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"svd\", TruncatedSVD(n_components=n_svd_components, random_state=SEED)),\n",
    "        ])\n",
    "\n",
    "    grids = {\n",
    "        \"logreg\": {\n",
    "            \"model\": LogisticRegression(class_weight=\"balanced\", multi_class=\"multinomial\", solver=\"saga\", max_iter=500, random_state=SEED, n_jobs=-1),\n",
    "            \"params\": {\"clf__C\": np.logspace(-3, 2, 10), \"clf__penalty\": [\"l1\", \"l2\"]}\n",
    "        },\n",
    "        \"svc_rbf\": {\n",
    "            \"model\": SVC(kernel=\"rbf\", probability=True, class_weight=\"balanced\", random_state=SEED),\n",
    "            \"params\": {\"clf__C\": np.logspace(-2, 2, 10), \"clf__gamma\": np.logspace(-3, 0, 10)}\n",
    "        },\n",
    "        \"knn\": {\n",
    "            \"model\": KNeighborsClassifier(n_jobs=-1),\n",
    "            \"params\": {\"clf__n_neighbors\": range(3, 12, 2), \"clf__weights\": [\"uniform\", \"distance\"]}\n",
    "        },\n",
    "        \"rf\": {\n",
    "            \"model\": RandomForestClassifier(class_weight=\"balanced_subsample\", n_jobs=-1, random_state=SEED),\n",
    "            \"params\": {\"clf__n_estimators\": [200, 300, 400], \"clf__max_depth\": [4, 6, 8, 10], \"clf__max_features\": [\"sqrt\", 0.3]}\n",
    "        },\n",
    "        \"xgb\": {\n",
    "            \"model\": xgb.XGBClassifier(objective=\"multi:softprob\", eval_metric=\"mlogloss\", n_jobs=-1, random_state=SEED, use_label_encoder=False, num_class=NUM_CLASSES),\n",
    "            \"params\": { \"clf__n_estimators\": [150, 250, 350], \"clf__max_depth\": [3, 4, 5], \"clf__learning_rate\": [0.05, 0.1],\n",
    "                        \"clf__subsample\": [0.7, 0.9], \"clf__colsample_bytree\": [0.6, 0.8]}\n",
    "        },\n",
    "        \"lgb\": {\n",
    "            \"model\": lgb.LGBMClassifier(objective=\"multiclass\", random_state=SEED, n_jobs=-1, n_estimators=250, num_class=NUM_CLASSES),\n",
    "            \"params\": {\"clf__num_leaves\": [31, 63], \"clf__max_depth\": [4, 6], \"clf__learning_rate\": [0.05, 0.1],\n",
    "                       \"clf__subsample\": [0.7, 0.9], \"clf__colsample_bytree\": [0.6, 0.8]}\n",
    "        },\n",
    "    }\n",
    "\n",
    "    skf_ml = StratifiedKFold(n_splits=N_SPLITS_ML, shuffle=True, random_state=SEED)\n",
    "    cv_results_current_esm = defaultdict(list)\n",
    "    scoring_for_search = \"f1_macro\" \n",
    "\n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(skf_ml.split(X_embeddings, y_labels), 1):\n",
    "        X_train_fold, X_test_fold = X_embeddings[train_idx], X_embeddings[test_idx]\n",
    "        y_train_ml, y_test_ml = y_labels[train_idx], y_labels[test_idx]\n",
    "        \n",
    "        print(f\"\\n===== ML Fold {fold_idx}/{N_SPLITS_ML} for {esm_name} =====\")\n",
    "        fold_fitted_models = {} \n",
    "\n",
    "        for model_name, cfg in grids.items():\n",
    "            print(f\"\\n  â³ Tuning {model_name} for {esm_name}, Fold {fold_idx}...\")\n",
    "            pipe = make_pipe(preproc_pipeline, cfg[\"model\"])\n",
    "            \n",
    "            search = RandomizedSearchCV(\n",
    "                estimator=pipe, param_distributions=cfg[\"params\"], n_iter=N_ITER_RANDOM_SEARCH,\n",
    "                scoring=scoring_for_search, cv=CV_RANDOM_SEARCH, random_state=SEED, n_jobs=-1, verbose=0\n",
    "            )\n",
    "            search.fit(X_train_fold, y_train_ml)\n",
    "            best_estimator_for_model = search.best_estimator_\n",
    "            fold_fitted_models[model_name] = best_estimator_for_model\n",
    "            preds = best_estimator_for_model.predict(X_test_fold)\n",
    "\n",
    "            macro_f1 = f1_score(y_test_ml, preds, average=\"macro\", zero_division=0)\n",
    "            kappa = cohen_kappa_score(y_test_ml, preds, weights=\"quadratic\")\n",
    "            \n",
    "            roc_auc_val = np.nan\n",
    "            if hasattr(best_estimator_for_model, \"predict_proba\"):\n",
    "                probas = best_estimator_for_model.predict_proba(X_test_fold)\n",
    "                unique_labels_in_fold = np.unique(y_test_ml)\n",
    "                if len(unique_labels_in_fold) == NUM_CLASSES:\n",
    "                    try:\n",
    "                        roc_auc_val = roc_auc_score(y_test_ml, probas, multi_class='ovr', average='weighted', labels=list(range(NUM_CLASSES)))\n",
    "                    except ValueError as e_roc:\n",
    "                        print(f\"    Warning: ROC AUC for {model_name} (Fold {fold_idx}) failed: {e_roc}\")\n",
    "                else:\n",
    "                    print(f\"    Info: ROC AUC for {model_name} (Fold {fold_idx}) N/A (only {len(unique_labels_in_fold)}/{NUM_CLASSES} classes in y_test_ml).\")\n",
    "            else:\n",
    "                print(f\"    Info: {model_name} does not have predict_proba. ROC AUC N/A.\")\n",
    "\n",
    "            cv_results_current_esm[f\"{model_name}_f1\"].append(macro_f1)\n",
    "            cv_results_current_esm[f\"{model_name}_kappa\"].append(kappa)\n",
    "            cv_results_current_esm[f\"{model_name}_roc_auc\"].append(roc_auc_val)\n",
    "            print(f\"    â†’ Best {model_name} (Fold {fold_idx}) Macro-F1: {macro_f1:.4f} / Kappa: {kappa:.4f} / ROC AUC: {roc_auc_val:.4f}\")\n",
    "\n",
    "        fold_model_performances = []\n",
    "        for model_name_key in fold_fitted_models.keys():\n",
    "            model_fold_f1 = cv_results_current_esm[f\"{model_name_key}_f1\"][-1] \n",
    "            fold_model_performances.append((model_name_key, model_fold_f1, fold_fitted_models[model_name_key]))\n",
    "        \n",
    "        top3_for_fold_ensemble = sorted(fold_model_performances, key=lambda item: item[1], reverse=True)[:3]\n",
    "        ensemble_estimators_this_fold = [(item[0], item[2]) for item in top3_for_fold_ensemble]\n",
    "        \n",
    "        if len(ensemble_estimators_this_fold) < 1:\n",
    "            print(f\"  ğŸš« Could not form ensemble for Fold {fold_idx}. Skipping.\")\n",
    "            cv_results_current_esm[\"ens_f1\"].append(np.nan)\n",
    "            cv_results_current_esm[\"ens_kappa\"].append(np.nan)\n",
    "            cv_results_current_esm[\"ens_roc_auc\"].append(np.nan)\n",
    "        else:\n",
    "            print(f\"  Ensemble for Fold {fold_idx} using: {[name for name, _ in ensemble_estimators_this_fold]}\")\n",
    "            ens = VotingClassifier(\n",
    "                estimators=ensemble_estimators_this_fold, voting=\"soft\", \n",
    "                weights=[3,2,1][:len(ensemble_estimators_this_fold)] \n",
    "            )\n",
    "            ens.fit(X_train_fold, y_train_ml)\n",
    "            ens_preds = ens.predict(X_test_fold)\n",
    "            \n",
    "            ens_macro_f1 = f1_score(y_test_ml, ens_preds, average=\"macro\", zero_division=0)\n",
    "            ens_kappa = cohen_kappa_score(y_test_ml, ens_preds, weights=\"quadratic\")\n",
    "\n",
    "            ens_roc_auc_val = np.nan\n",
    "            if hasattr(ens, \"predict_proba\"):\n",
    "                ens_probas = ens.predict_proba(X_test_fold)\n",
    "                unique_labels_in_fold_ens = np.unique(y_test_ml)\n",
    "                if len(unique_labels_in_fold_ens) == NUM_CLASSES:\n",
    "                    try:\n",
    "                        ens_roc_auc_val = roc_auc_score(y_test_ml, ens_probas, multi_class='ovr', average='weighted', labels=list(range(NUM_CLASSES)))\n",
    "                    except ValueError as e_roc_ens:\n",
    "                        print(f\"    Warning: ROC AUC for Ensemble (Fold {fold_idx}) failed: {e_roc_ens}\")\n",
    "                else:\n",
    "                    print(f\"    Info: ROC AUC for Ensemble (Fold {fold_idx}) N/A (only {len(unique_labels_in_fold_ens)}/{NUM_CLASSES} classes in y_test_ml).\")\n",
    "            else:\n",
    "                print(f\"    Warning: Ensemble does not have predict_proba. ROC AUC N/A.\")\n",
    "            \n",
    "            cv_results_current_esm[\"ens_f1\"].append(ens_macro_f1)\n",
    "            cv_results_current_esm[\"ens_kappa\"].append(ens_kappa)\n",
    "            cv_results_current_esm[\"ens_roc_auc\"].append(ens_roc_auc_val)\n",
    "            print(f\"  âœ… Ensemble (Fold {fold_idx}) Macro-F1: {ens_macro_f1:.4f} / Kappa: {ens_kappa:.4f} / ROC AUC: {ens_roc_auc_val:.4f}\")\n",
    "        \n",
    "        del X_train_fold, X_test_fold, y_train_ml, y_test_ml, fold_fitted_models \n",
    "        if 'ens' in locals(): del ens \n",
    "        gc.collect()\n",
    "\n",
    "    print(f\"\\n===== Averaged Results for {esm_name} (over {N_SPLITS_ML} ML folds) =====\")\n",
    "    esm_summary = {\"ESM Model\": esm_name}\n",
    "    model_keys_for_summary = list(grids.keys()) + [\"ens\"]\n",
    "    for model_key in model_keys_for_summary:\n",
    "        mean_f1 = np.nanmean(cv_results_current_esm.get(f\"{model_key}_f1\", [np.nan]))\n",
    "        std_f1 = np.nanstd(cv_results_current_esm.get(f\"{model_key}_f1\", [np.nan]))\n",
    "        mean_kappa = np.nanmean(cv_results_current_esm.get(f\"{model_key}_kappa\", [np.nan]))\n",
    "        std_kappa = np.nanstd(cv_results_current_esm.get(f\"{model_key}_kappa\", [np.nan]))\n",
    "        mean_roc_auc = np.nanmean(cv_results_current_esm.get(f\"{model_key}_roc_auc\", [np.nan]))\n",
    "        std_roc_auc = np.nanstd(cv_results_current_esm.get(f\"{model_key}_roc_auc\", [np.nan]))\n",
    "        \n",
    "        print(f\"  {model_key:<8} Macro-F1 = {mean_f1:.4f} Â± {std_f1:.4f} | Kappa = {mean_kappa:.4f} Â± {std_kappa:.4f} | ROC AUC = {mean_roc_auc:.4f} Â± {std_roc_auc:.4f}\")\n",
    "        if model_key == \"ens\": \n",
    "            esm_summary[\"Ensemble Macro F1 Mean\"] = mean_f1\n",
    "            esm_summary[\"Ensemble Macro F1 Std\"] = std_f1\n",
    "            esm_summary[\"Ensemble Kappa Mean\"] = mean_kappa\n",
    "            esm_summary[\"Ensemble Kappa Std\"] = std_kappa\n",
    "            esm_summary[\"Ensemble ROC AUC Mean\"] = mean_roc_auc\n",
    "            esm_summary[\"Ensemble ROC AUC Std\"] = std_roc_auc\n",
    "            \n",
    "    overall_esm_results.append(esm_summary)\n",
    "    del X_embeddings, cv_results_current_esm \n",
    "    gc.collect()\n",
    "    if device.type == 'cuda': torch.cuda.empty_cache()\n",
    "    if device.type == 'mps': torch.mps.empty_cache()\n",
    "\n",
    "print(\"\\n\\n{'='*30} Overall Summary Across ESM Models {'='*30}\")\n",
    "summary_df = pd.DataFrame(overall_esm_results)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "print(summary_df.round(4).to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
